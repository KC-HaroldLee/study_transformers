{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë³¸ ë…¸íŠ¸ë¶ íŒŒì¼ì€ ë°©ê¸ˆ í‡´ê·¼í•˜ê³  ëŸ¬ë‹ê¹Œì§€ í•˜ê³  ì˜¨ \"ë¬¸ì†¡í•œ ê°œë°œì\"ì˜ ê¸°ë¡ì…ë‹ˆë‹¤.<br>ğŸ˜¼ìƒë‹¹íˆ ë§ì€ í­ë ¥ì ì¸ í‘œí˜„ì´ í‘œí˜„ë˜ì–´ ìˆìœ¼ë©° ê´‘ê¸° ë„˜ì¹˜ëŠ” ë¬¸ì¥ì´ ë²”ëŒí•˜ê³  ìˆìœ¼ë¯€ë¡œ ì—´ëŒì— ì£¼ì˜í•˜ì—¬ì£¼ì„¸ìš”<br>ì˜¤ëŠ˜ì˜ ìŠ¤íŠ¸ë ˆìŠ¤ ì§€ìˆ˜ <font color='red'>95</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kk4ever/anaconda3/envs/hg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ì°¨ë¶„í•˜ê²Œ ë‹¤ì‹œ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ì..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ìƒ˜í”Œ ë¬¸ì¥ë„ ì±…ì— ìˆëŠ” ê±°ë£¨..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_1 = \"tokenizing text is a core task of NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(sample_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì•„ ë§ë‹¤ ê·¸ëƒ¥ ì½œí•˜ë©´ ì–´í…ì…˜ë§ˆìŠ¤í¬ë„ ë‚˜ì˜¨ë‹¤. ë‚´ë¶€ì—ì„œëŠ” encode_plus()ê°€ ì½œëœë‹¤ê³  ë³´ë©´ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102]\n",
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(sample_text_1))\n",
    "print(tokenizer(sample_text_1))\n",
    "print(tokenizer.encode_plus(sample_text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ë­ ê·¼ë° tokení™” í•˜ëŠ” ë°ì—ëŠ” ...input_idsë§Œ ì“°ê¸´í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(sample_text_1).input_ids)\n",
    "print(tokens)\n",
    "\n",
    "# ë‹¹ì—° ì•„ë˜ ê±´ ì•ˆë˜ê³ \n",
    "# tokenizer.convert_ids_to_tokens(tokenizer(sample_text_1))\n",
    "\n",
    "# ì´ê±´ ëœë‹¤. encode()ëŠ” input_idsë§Œ ë°˜í™˜í•˜ë‹ˆê¹Œ\n",
    "# tokenizer.convert_ids_to_tokens(tokenizer.encode(sample_text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì´ëŸ¬ë©´ ì´ì˜ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp [SEP]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))\n",
    "print(type(tokenizer.convert_tokens_to_string(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vocabì‚¬ì´ì¦ˆë¥¼ ë³´ê¸°ìœ„í•´ configíŒŒì¼ì„ êµ³ì´ ë³¼ í•„ìš”ê°€ ì—†ìœ¼ë©°, ë‹¹ì—° vocabì‚¬ì´ì¦ˆë¥¼ ë³´ê¸°ìœ„í•´ modelë¥¼ ë¶ˆëŸ¬ ì˜¬ í•„ìš”ëŠ” ì—†ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 30522\n",
      "model_max_length 512\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size', tokenizer.vocab_size)\n",
    "print('model_max_length', tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### íŠ¹ì´í•œ ê²½ìš°ì´ë‹¤. ì´ ì™¸ì— ë‹¤ë¥¸ ë°˜í™˜í˜•íƒœê°€ ìˆì„ ìˆ˜ë„ ìˆë‹¤. ì–¸ì œë‚˜ ì£¼ì‹œí• ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì•„ë‹ˆ ì§€ê¸ˆ ì´ ìë¦¬ì—ì„œ ë°í˜€ ë‚´ë³´ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŒíŠ¸ë¥¼ ì–»ê¸°ìœ„í•´ í´ë˜ìŠ¤ ì´ë¦„ì„ ë½‘ì•„ë‚´ê³ \n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.tokenization_distilbert_fast import DistilBertTokenizerFast\n",
    "# model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "# ë¥¼ ì°¾ê³ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "# ì—ëŠ” \n",
    "# model_input_names: List[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]ë¡œ ë˜ì–´ ìˆë‹¤.\n",
    "# ì•„ë¬´íŠ¼ ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ê³  í™•ì¸í–ˆìœ¼ë‹ˆ ì—¬ê¸°ê¹Œì§€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
