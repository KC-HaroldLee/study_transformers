{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 본 노트북 파일은 방금 퇴근하고 러닝까지 하고 온 \"문송한 개발자\"의 기록입니다.<br>😼상당히 많은 폭력적인 표현이 표현되어 있으며 광기 넘치는 문장이 범람하고 있으므로 열람에 주의하여주세요<br>오늘의 스트레스 지수 <font color='red'>95</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kk4ever/anaconda3/envs/hg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  차분하게 다시 토크나이저를 불러오자..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 샘플 문장도 책에 있는 거루..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_1 = \"tokenizing text is a core task of NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(sample_text_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아 맞다 그냥 콜하면 어텐션마스크도 나온다. 내부에서는 encode_plus()가 콜된다고 보면된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102]\n",
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(sample_text_1))\n",
    "print(tokenizer(sample_text_1))\n",
    "print(tokenizer.encode_plus(sample_text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 뭐 근데 token화 하는 데에는 ...input_ids만 쓰긴한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(sample_text_1).input_ids)\n",
    "print(tokens)\n",
    "\n",
    "# 당연 아래 건 안되고\n",
    "# tokenizer.convert_ids_to_tokens(tokenizer(sample_text_1))\n",
    "\n",
    "# 이건 된다. encode()는 input_ids만 반환하니까\n",
    "# tokenizer.convert_ids_to_tokens(tokenizer.encode(sample_text_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이러면 이쁘다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp [SEP]\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))\n",
    "print(type(tokenizer.convert_tokens_to_string(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vocab사이즈를 보기위해 config파일을 굳이 볼 필요가 없으며, 당연 vocab사이즈를 보기위해 model를 불러 올 필요는 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 30522\n",
      "model_max_length 512\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size', tokenizer.vocab_size)\n",
    "print('model_max_length', tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 특이한 경우이다. 이 외에 다른 반환형태가 있을 수도 있다. 언제나 주시할것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 아니 지금 이 자리에서 밝혀 내보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 힌트를 얻기위해 클래스 이름을 뽑아내고\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.tokenization_distilbert_fast import DistilBertTokenizerFast\n",
    "# model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "# 를 찾고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "# 에는 \n",
    "# model_input_names: List[str] = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]로 되어 있다.\n",
    "# 아무튼 모델마다 다를 수 있다고 확인했으니 여기까지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
